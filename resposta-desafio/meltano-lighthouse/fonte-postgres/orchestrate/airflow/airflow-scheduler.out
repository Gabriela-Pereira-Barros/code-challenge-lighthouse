  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-07-03T17:22:07.162-0300[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-07-03T17:22:07.163-0300[0m] {[34mexecutor_loader.py:[0m115} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-07-03T17:22:07.181-0300[0m] {[34mscheduler_job_runner.py:[0m808} INFO[0m - Starting the scheduler[0m
[[34m2024-07-03T17:22:07.181-0300[0m] {[34mscheduler_job_runner.py:[0m815} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-07-03T17:22:07.184-0300[0m] {[34mmanager.py:[0m169} INFO[0m - Launched DagFileProcessorManager with pid: 185569[0m
[[34m2024-07-03T17:22:07.185-0300[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-03T17:22:07.186-0300[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-07-03T17:22:07.196-0300] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-07-03T17:22:07.336-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:22:00+00:00, run_after=2024-07-03 20:23:00+00:00[0m
[[34m2024-07-03T17:22:07.423-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:21:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:22:07.424-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:22:07.424-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:21:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:22:07.425-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:22:07.425-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:21:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:22:07.425-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:21:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:22:07.428-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:21:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:22:08.030-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:21:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:22:08.113-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:21:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:22:12.923-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:21:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:22:12.930-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:21:00+00:00, map_index=-1, run_start_date=2024-07-03 20:22:08.136819+00:00, run_end_date=2024-07-03 20:22:12.701099+00:00, run_duration=4.56428, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:22:07.424402+00:00, queued_by_job_id=1, pid=185577[0m
[[34m2024-07-03T17:22:14.674-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:21:00+00:00: scheduled__2024-07-03T20:21:00+00:00, state:running, queued_at: 2024-07-03 20:22:07.328561+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:22:14.675-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:21:00+00:00, run_id=scheduled__2024-07-03T20:21:00+00:00, run_start_date=2024-07-03 20:22:07.401600+00:00, run_end_date=2024-07-03 20:22:14.675453+00:00, run_duration=7.273853, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:21:00+00:00, data_interval_end=2024-07-03 20:22:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:22:14.679-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:22:00+00:00, run_after=2024-07-03 20:23:00+00:00[0m
[[34m2024-07-03T17:23:01.662-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:23:00+00:00, run_after=2024-07-03 20:24:00+00:00[0m
[[34m2024-07-03T17:23:01.686-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:22:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:23:01.686-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:23:01.686-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:22:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:23:01.687-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:23:01.688-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:22:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:23:01.688-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:22:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:23:01.691-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:22:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:23:02.360-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:22:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:23:02.431-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:22:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:23:06.115-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:22:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:23:06.118-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:22:00+00:00, map_index=-1, run_start_date=2024-07-03 20:23:02.459944+00:00, run_end_date=2024-07-03 20:23:05.858233+00:00, run_duration=3.398289, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:23:01.687243+00:00, queued_by_job_id=1, pid=185918[0m
[[34m2024-07-03T17:23:06.157-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:22:00+00:00: scheduled__2024-07-03T20:22:00+00:00, state:running, queued_at: 2024-07-03 20:23:01.658920+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:23:06.158-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:22:00+00:00, run_id=scheduled__2024-07-03T20:22:00+00:00, run_start_date=2024-07-03 20:23:01.670992+00:00, run_end_date=2024-07-03 20:23:06.158261+00:00, run_duration=4.487269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:22:00+00:00, data_interval_end=2024-07-03 20:23:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:23:06.163-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:23:00+00:00, run_after=2024-07-03 20:24:00+00:00[0m
[[34m2024-07-03T17:24:01.376-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:24:00+00:00, run_after=2024-07-03 20:25:00+00:00[0m
[[34m2024-07-03T17:24:01.403-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:23:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:24:01.403-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:24:01.403-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:23:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:24:01.404-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:24:01.404-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:23:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:24:01.404-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:23:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:24:01.408-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:23:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:24:02.060-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:23:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:24:02.133-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:23:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:24:05.886-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:23:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:24:05.890-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:23:00+00:00, map_index=-1, run_start_date=2024-07-03 20:24:02.173492+00:00, run_end_date=2024-07-03 20:24:05.617716+00:00, run_duration=3.444224, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:24:01.403966+00:00, queued_by_job_id=1, pid=186304[0m
[[34m2024-07-03T17:24:05.930-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:23:00+00:00: scheduled__2024-07-03T20:23:00+00:00, state:running, queued_at: 2024-07-03 20:24:01.372267+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:24:05.931-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:23:00+00:00, run_id=scheduled__2024-07-03T20:23:00+00:00, run_start_date=2024-07-03 20:24:01.384942+00:00, run_end_date=2024-07-03 20:24:05.931455+00:00, run_duration=4.546513, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:23:00+00:00, data_interval_end=2024-07-03 20:24:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:24:05.938-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:24:00+00:00, run_after=2024-07-03 20:25:00+00:00[0m
[[34m2024-07-03T17:25:01.147-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:25:00+00:00, run_after=2024-07-03 20:26:00+00:00[0m
[[34m2024-07-03T17:25:01.168-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:24:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:25:01.168-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:25:01.168-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:24:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:25:01.169-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:25:01.169-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:24:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:25:01.169-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:24:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:25:01.172-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:24:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:25:01.783-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:24:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:25:01.851-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:24:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:25:05.478-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:24:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:25:05.481-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:24:00+00:00, map_index=-1, run_start_date=2024-07-03 20:25:01.875170+00:00, run_end_date=2024-07-03 20:25:05.246087+00:00, run_duration=3.370917, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:25:01.169032+00:00, queued_by_job_id=1, pid=186716[0m
[[34m2024-07-03T17:25:05.517-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:24:00+00:00: scheduled__2024-07-03T20:24:00+00:00, state:running, queued_at: 2024-07-03 20:25:01.145000+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:25:05.517-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:24:00+00:00, run_id=scheduled__2024-07-03T20:24:00+00:00, run_start_date=2024-07-03 20:25:01.155259+00:00, run_end_date=2024-07-03 20:25:05.517537+00:00, run_duration=4.362278, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:24:00+00:00, data_interval_end=2024-07-03 20:25:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:25:05.522-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:25:00+00:00, run_after=2024-07-03 20:26:00+00:00[0m
[[34m2024-07-03T17:26:02.015-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:26:00+00:00, run_after=2024-07-03 20:27:00+00:00[0m
[[34m2024-07-03T17:26:02.037-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:25:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:26:02.037-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:26:02.038-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:25:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:26:02.038-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:26:02.038-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:25:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:26:02.039-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:26:02.042-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:26:02.723-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:25:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:26:02.795-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:25:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:26:06.602-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:26:06.606-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:25:00+00:00, map_index=-1, run_start_date=2024-07-03 20:26:02.824424+00:00, run_end_date=2024-07-03 20:26:06.312896+00:00, run_duration=3.488472, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:26:02.038306+00:00, queued_by_job_id=1, pid=187086[0m
[[34m2024-07-03T17:26:06.637-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:25:00+00:00: scheduled__2024-07-03T20:25:00+00:00, state:running, queued_at: 2024-07-03 20:26:02.011734+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:26:06.637-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:25:00+00:00, run_id=scheduled__2024-07-03T20:25:00+00:00, run_start_date=2024-07-03 20:26:02.023435+00:00, run_end_date=2024-07-03 20:26:06.637326+00:00, run_duration=4.613891, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:25:00+00:00, data_interval_end=2024-07-03 20:26:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:26:06.639-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:26:00+00:00, run_after=2024-07-03 20:27:00+00:00[0m
[[34m2024-07-03T17:27:01.118-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:27:00+00:00, run_after=2024-07-03 20:28:00+00:00[0m
[[34m2024-07-03T17:27:01.154-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:26:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:27:01.154-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:27:01.155-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:26:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:27:01.156-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:27:01.156-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:26:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:27:01.157-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:26:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:27:01.160-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:26:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:27:01.834-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:26:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:27:01.904-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:26:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:27:05.850-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:26:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:27:05.855-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:26:00+00:00, map_index=-1, run_start_date=2024-07-03 20:27:01.931988+00:00, run_end_date=2024-07-03 20:27:05.606012+00:00, run_duration=3.674024, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:27:01.155535+00:00, queued_by_job_id=1, pid=187460[0m
[[34m2024-07-03T17:27:07.702-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:26:00+00:00: scheduled__2024-07-03T20:26:00+00:00, state:running, queued_at: 2024-07-03 20:27:01.113823+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:27:07.703-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:26:00+00:00, run_id=scheduled__2024-07-03T20:26:00+00:00, run_start_date=2024-07-03 20:27:01.131736+00:00, run_end_date=2024-07-03 20:27:07.702934+00:00, run_duration=6.571198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:26:00+00:00, data_interval_end=2024-07-03 20:27:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:27:07.707-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:27:00+00:00, run_after=2024-07-03 20:28:00+00:00[0m
[[34m2024-07-03T17:27:07.732-0300[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-03T17:28:01.256-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:28:00+00:00, run_after=2024-07-03 20:29:00+00:00[0m
[[34m2024-07-03T17:28:01.281-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:27:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:28:01.282-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:28:01.282-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:27:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:28:01.283-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:28:01.283-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:27:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:28:01.283-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:27:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:28:01.287-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:27:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:28:01.998-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:27:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:28:02.081-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:27:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:28:08.463-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:27:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:28:08.470-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:27:00+00:00, map_index=-1, run_start_date=2024-07-03 20:28:02.113060+00:00, run_end_date=2024-07-03 20:28:08.204802+00:00, run_duration=6.091742, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:28:01.282556+00:00, queued_by_job_id=1, pid=188009[0m
[[34m2024-07-03T17:28:08.523-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:27:00+00:00: scheduled__2024-07-03T20:27:00+00:00, state:running, queued_at: 2024-07-03 20:28:01.251918+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:28:08.524-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:27:00+00:00, run_id=scheduled__2024-07-03T20:27:00+00:00, run_start_date=2024-07-03 20:28:01.265222+00:00, run_end_date=2024-07-03 20:28:08.524298+00:00, run_duration=7.259076, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:27:00+00:00, data_interval_end=2024-07-03 20:28:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:28:08.528-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:28:00+00:00, run_after=2024-07-03 20:29:00+00:00[0m
[[34m2024-07-03T17:29:01.344-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:29:00+00:00, run_after=2024-07-03 20:30:00+00:00[0m
[[34m2024-07-03T17:29:01.374-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:28:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:29:01.374-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:29:01.374-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:28:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:29:01.375-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:29:01.376-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:28:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:29:01.376-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:28:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:29:01.379-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:28:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:29:02.154-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:28:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:29:02.314-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:28:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:29:06.224-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:28:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:29:06.229-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:28:00+00:00, map_index=-1, run_start_date=2024-07-03 20:29:02.360874+00:00, run_end_date=2024-07-03 20:29:05.958180+00:00, run_duration=3.597306, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:29:01.375058+00:00, queued_by_job_id=1, pid=188499[0m
[[34m2024-07-03T17:29:06.270-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:28:00+00:00: scheduled__2024-07-03T20:28:00+00:00, state:running, queued_at: 2024-07-03 20:29:01.338877+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:29:06.271-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:28:00+00:00, run_id=scheduled__2024-07-03T20:28:00+00:00, run_start_date=2024-07-03 20:29:01.354421+00:00, run_end_date=2024-07-03 20:29:06.270943+00:00, run_duration=4.916522, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:28:00+00:00, data_interval_end=2024-07-03 20:29:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:29:06.275-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:29:00+00:00, run_after=2024-07-03 20:30:00+00:00[0m
[[34m2024-07-03T17:30:01.622-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:30:00+00:00, run_after=2024-07-03 20:31:00+00:00[0m
[[34m2024-07-03T17:30:01.650-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:29:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:30:01.651-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:30:01.651-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:29:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:30:01.652-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:30:01.652-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:29:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:30:01.652-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:29:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:30:01.656-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:29:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:30:02.454-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:29:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:30:02.526-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:29:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:30:06.438-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:29:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:30:06.447-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:29:00+00:00, map_index=-1, run_start_date=2024-07-03 20:30:02.554337+00:00, run_end_date=2024-07-03 20:30:06.117495+00:00, run_duration=3.563158, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:30:01.651835+00:00, queued_by_job_id=1, pid=188867[0m
[[34m2024-07-03T17:30:06.484-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:29:00+00:00: scheduled__2024-07-03T20:29:00+00:00, state:running, queued_at: 2024-07-03 20:30:01.618664+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:30:06.484-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:29:00+00:00, run_id=scheduled__2024-07-03T20:29:00+00:00, run_start_date=2024-07-03 20:30:01.632584+00:00, run_end_date=2024-07-03 20:30:06.484564+00:00, run_duration=4.85198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:29:00+00:00, data_interval_end=2024-07-03 20:30:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:30:06.486-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:30:00+00:00, run_after=2024-07-03 20:31:00+00:00[0m
[[34m2024-07-03T17:31:01.777-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:31:00+00:00, run_after=2024-07-03 20:32:00+00:00[0m
[[34m2024-07-03T17:31:01.807-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:30:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:31:01.808-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:31:01.808-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:30:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:31:01.809-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:31:01.810-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:31:01.810-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:31:01.814-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:31:02.632-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:30:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:31:02.720-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:30:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:31:06.754-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:31:06.761-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:30:00+00:00, map_index=-1, run_start_date=2024-07-03 20:31:02.756377+00:00, run_end_date=2024-07-03 20:31:06.532014+00:00, run_duration=3.775637, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:31:01.809042+00:00, queued_by_job_id=1, pid=189302[0m
[[34m2024-07-03T17:31:06.807-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:30:00+00:00: scheduled__2024-07-03T20:30:00+00:00, state:running, queued_at: 2024-07-03 20:31:01.774298+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:31:06.807-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:30:00+00:00, run_id=scheduled__2024-07-03T20:30:00+00:00, run_start_date=2024-07-03 20:31:01.785597+00:00, run_end_date=2024-07-03 20:31:06.807536+00:00, run_duration=5.021939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:30:00+00:00, data_interval_end=2024-07-03 20:31:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:31:06.812-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:31:00+00:00, run_after=2024-07-03 20:32:00+00:00[0m
[[34m2024-07-03T17:32:01.783-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:32:00+00:00, run_after=2024-07-03 20:33:00+00:00[0m
[[34m2024-07-03T17:32:01.840-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:31:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:32:01.840-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:32:01.841-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:31:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:32:01.842-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:32:01.843-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:31:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:32:01.843-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:31:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:32:01.847-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:31:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:32:02.769-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:31:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:32:02.858-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:31:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:32:07.317-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:31:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:32:07.321-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:31:00+00:00, map_index=-1, run_start_date=2024-07-03 20:32:02.904493+00:00, run_end_date=2024-07-03 20:32:07.042276+00:00, run_duration=4.137783, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:32:01.841824+00:00, queued_by_job_id=1, pid=189650[0m
[[34m2024-07-03T17:32:07.351-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:31:00+00:00: scheduled__2024-07-03T20:31:00+00:00, state:running, queued_at: 2024-07-03 20:32:01.769517+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:32:07.351-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:31:00+00:00, run_id=scheduled__2024-07-03T20:31:00+00:00, run_start_date=2024-07-03 20:32:01.798363+00:00, run_end_date=2024-07-03 20:32:07.351757+00:00, run_duration=5.553394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:31:00+00:00, data_interval_end=2024-07-03 20:32:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:32:07.354-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:32:00+00:00, run_after=2024-07-03 20:33:00+00:00[0m
[[34m2024-07-03T17:32:07.773-0300[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-03T17:33:01.932-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:33:00+00:00, run_after=2024-07-03 20:34:00+00:00[0m
[[34m2024-07-03T17:33:02.000-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:32:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:33:02.001-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:33:02.001-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:32:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:33:02.003-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:33:02.004-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:32:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:33:02.004-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:32:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:33:02.011-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:32:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:33:02.891-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:32:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:33:02.971-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:32:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:33:06.755-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:32:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:33:06.760-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:32:00+00:00, map_index=-1, run_start_date=2024-07-03 20:33:03.001101+00:00, run_end_date=2024-07-03 20:33:06.481180+00:00, run_duration=3.480079, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:33:02.002080+00:00, queued_by_job_id=1, pid=190011[0m
[[34m2024-07-03T17:33:06.789-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:32:00+00:00: scheduled__2024-07-03T20:32:00+00:00, state:running, queued_at: 2024-07-03 20:33:01.909468+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:33:06.789-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:32:00+00:00, run_id=scheduled__2024-07-03T20:32:00+00:00, run_start_date=2024-07-03 20:33:01.972793+00:00, run_end_date=2024-07-03 20:33:06.789466+00:00, run_duration=4.816673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:32:00+00:00, data_interval_end=2024-07-03 20:33:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:33:06.791-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:33:00+00:00, run_after=2024-07-03 20:34:00+00:00[0m
[[34m2024-07-03T17:34:01.317-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:34:00+00:00, run_after=2024-07-03 20:35:00+00:00[0m
[[34m2024-07-03T17:34:01.354-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:33:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:34:01.354-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:34:01.355-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:33:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:34:01.356-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:34:01.357-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:33:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:34:01.357-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:33:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:34:01.360-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:33:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:34:02.708-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:33:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:34:02.896-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:33:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:34:07.484-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:33:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:34:07.490-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:33:00+00:00, map_index=-1, run_start_date=2024-07-03 20:34:02.951266+00:00, run_end_date=2024-07-03 20:34:07.270547+00:00, run_duration=4.319281, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:34:01.355622+00:00, queued_by_job_id=1, pid=190403[0m
[[34m2024-07-03T17:34:07.517-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:33:00+00:00: scheduled__2024-07-03T20:33:00+00:00, state:running, queued_at: 2024-07-03 20:34:01.311015+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:34:07.517-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:33:00+00:00, run_id=scheduled__2024-07-03T20:33:00+00:00, run_start_date=2024-07-03 20:34:01.329040+00:00, run_end_date=2024-07-03 20:34:07.517676+00:00, run_duration=6.188636, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:33:00+00:00, data_interval_end=2024-07-03 20:34:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:34:07.519-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:34:00+00:00, run_after=2024-07-03 20:35:00+00:00[0m
[[34m2024-07-03T17:35:01.203-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:35:00+00:00, run_after=2024-07-03 20:36:00+00:00[0m
[[34m2024-07-03T17:35:01.237-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:34:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:35:01.237-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:35:01.237-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:34:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:35:01.239-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:35:01.239-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:34:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:35:01.239-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:34:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:35:01.243-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:34:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:35:01.973-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:34:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:35:02.043-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:34:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:35:05.719-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:34:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:35:05.724-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:34:00+00:00, map_index=-1, run_start_date=2024-07-03 20:35:02.071252+00:00, run_end_date=2024-07-03 20:35:05.387400+00:00, run_duration=3.316148, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:35:01.238291+00:00, queued_by_job_id=1, pid=190822[0m
[[34m2024-07-03T17:35:05.761-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:34:00+00:00: scheduled__2024-07-03T20:34:00+00:00, state:running, queued_at: 2024-07-03 20:35:01.198852+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:35:05.761-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:34:00+00:00, run_id=scheduled__2024-07-03T20:34:00+00:00, run_start_date=2024-07-03 20:35:01.213622+00:00, run_end_date=2024-07-03 20:35:05.761372+00:00, run_duration=4.54775, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:34:00+00:00, data_interval_end=2024-07-03 20:35:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:35:05.764-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:35:00+00:00, run_after=2024-07-03 20:36:00+00:00[0m
[[34m2024-07-03T17:36:02.934-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:36:00+00:00, run_after=2024-07-03 20:37:00+00:00[0m
[[34m2024-07-03T17:36:02.965-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:35:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:36:02.966-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:36:02.966-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:35:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:36:02.968-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:36:02.968-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:35:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:36:02.968-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:36:02.972-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:36:03.711-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:35:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:36:03.814-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:35:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:36:07.838-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:36:07.842-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:35:00+00:00, map_index=-1, run_start_date=2024-07-03 20:36:03.854899+00:00, run_end_date=2024-07-03 20:36:07.611874+00:00, run_duration=3.756975, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:36:02.967153+00:00, queued_by_job_id=1, pid=191210[0m
[[34m2024-07-03T17:36:07.981-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:35:00+00:00: scheduled__2024-07-03T20:35:00+00:00, state:running, queued_at: 2024-07-03 20:36:02.931270+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:36:07.981-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:35:00+00:00, run_id=scheduled__2024-07-03T20:35:00+00:00, run_start_date=2024-07-03 20:36:02.942229+00:00, run_end_date=2024-07-03 20:36:07.981666+00:00, run_duration=5.039437, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:35:00+00:00, data_interval_end=2024-07-03 20:36:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:36:07.985-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:36:00+00:00, run_after=2024-07-03 20:37:00+00:00[0m
[[34m2024-07-03T17:37:01.175-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:37:00+00:00, run_after=2024-07-03 20:38:00+00:00[0m
[[34m2024-07-03T17:37:01.204-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:36:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:37:01.204-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:37:01.204-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:36:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:37:01.205-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:37:01.206-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:36:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:37:01.206-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:36:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:37:01.209-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:36:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:37:01.993-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:36:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:37:02.083-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:36:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:37:06.504-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:36:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:37:06.509-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:36:00+00:00, map_index=-1, run_start_date=2024-07-03 20:37:02.117707+00:00, run_end_date=2024-07-03 20:37:06.220503+00:00, run_duration=4.102796, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:37:01.204939+00:00, queued_by_job_id=1, pid=191563[0m
[[34m2024-07-03T17:37:08.667-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:36:00+00:00: scheduled__2024-07-03T20:36:00+00:00, state:running, queued_at: 2024-07-03 20:37:01.171999+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:37:08.668-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:36:00+00:00, run_id=scheduled__2024-07-03T20:36:00+00:00, run_start_date=2024-07-03 20:37:01.186006+00:00, run_end_date=2024-07-03 20:37:08.667978+00:00, run_duration=7.481972, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:36:00+00:00, data_interval_end=2024-07-03 20:37:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:37:08.675-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:37:00+00:00, run_after=2024-07-03 20:38:00+00:00[0m
[[34m2024-07-03T17:37:08.698-0300[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-03T17:38:02.050-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:38:00+00:00, run_after=2024-07-03 20:39:00+00:00[0m
[[34m2024-07-03T17:38:02.096-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:37:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:38:02.096-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:38:02.097-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:37:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:38:02.098-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:38:02.099-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:37:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:38:02.099-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:37:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:38:02.103-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:37:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:38:02.826-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:37:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:38:02.902-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:37:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:38:06.703-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:37:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:38:06.706-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:37:00+00:00, map_index=-1, run_start_date=2024-07-03 20:38:02.931568+00:00, run_end_date=2024-07-03 20:38:06.502882+00:00, run_duration=3.571314, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:38:02.097703+00:00, queued_by_job_id=1, pid=191974[0m
[[34m2024-07-03T17:38:06.736-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:37:00+00:00: scheduled__2024-07-03T20:37:00+00:00, state:running, queued_at: 2024-07-03 20:38:02.044198+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:38:06.737-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:37:00+00:00, run_id=scheduled__2024-07-03T20:37:00+00:00, run_start_date=2024-07-03 20:38:02.065309+00:00, run_end_date=2024-07-03 20:38:06.737183+00:00, run_duration=4.671874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:37:00+00:00, data_interval_end=2024-07-03 20:38:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:38:06.741-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:38:00+00:00, run_after=2024-07-03 20:39:00+00:00[0m
[[34m2024-07-03T17:39:01.145-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:39:00+00:00, run_after=2024-07-03 20:40:00+00:00[0m
[[34m2024-07-03T17:39:01.183-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:38:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:39:01.184-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:39:01.184-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:38:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:39:01.185-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:39:01.185-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:38:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:39:01.185-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:38:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:39:01.188-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:38:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:39:01.800-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:38:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:39:01.878-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:38:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:39:05.318-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:38:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:39:05.322-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:38:00+00:00, map_index=-1, run_start_date=2024-07-03 20:39:01.903432+00:00, run_end_date=2024-07-03 20:39:05.131304+00:00, run_duration=3.227872, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:39:01.184685+00:00, queued_by_job_id=1, pid=192456[0m
[[34m2024-07-03T17:39:05.351-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:38:00+00:00: scheduled__2024-07-03T20:38:00+00:00, state:running, queued_at: 2024-07-03 20:39:01.138612+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:39:05.351-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:38:00+00:00, run_id=scheduled__2024-07-03T20:38:00+00:00, run_start_date=2024-07-03 20:39:01.159885+00:00, run_end_date=2024-07-03 20:39:05.351583+00:00, run_duration=4.191698, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:38:00+00:00, data_interval_end=2024-07-03 20:39:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:39:05.353-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:39:00+00:00, run_after=2024-07-03 20:40:00+00:00[0m
[[34m2024-07-03T17:40:01.239-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:40:00+00:00, run_after=2024-07-03 20:41:00+00:00[0m
[[34m2024-07-03T17:40:01.268-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:39:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:40:01.268-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:40:01.269-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:39:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:40:01.271-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:40:01.271-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:39:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:40:01.272-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:39:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:40:01.276-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:39:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:40:01.940-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:39:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:40:02.012-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:39:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:40:05.721-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:39:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:40:05.725-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:39:00+00:00, map_index=-1, run_start_date=2024-07-03 20:40:02.039449+00:00, run_end_date=2024-07-03 20:40:05.458873+00:00, run_duration=3.419424, state=success, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:40:01.269965+00:00, queued_by_job_id=1, pid=192854[0m
[[34m2024-07-03T17:40:05.764-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:39:00+00:00: scheduled__2024-07-03T20:39:00+00:00, state:running, queued_at: 2024-07-03 20:40:01.234692+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:40:05.765-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:39:00+00:00, run_id=scheduled__2024-07-03T20:39:00+00:00, run_start_date=2024-07-03 20:40:01.248938+00:00, run_end_date=2024-07-03 20:40:05.765198+00:00, run_duration=4.51626, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:39:00+00:00, data_interval_end=2024-07-03 20:40:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:40:05.770-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:40:00+00:00, run_after=2024-07-03 20:41:00+00:00[0m
[[34m2024-07-03T17:41:01.361-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:41:00+00:00, run_after=2024-07-03 20:42:00+00:00[0m
[[34m2024-07-03T17:41:01.383-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:40:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:41:01.383-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:41:01.383-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:40:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:41:01.384-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:41:01.385-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:41:01.385-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:41:01.388-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:41:02.054-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:40:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:41:02.131-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:40:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:41:05.880-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:41:05.883-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:40:00+00:00, map_index=-1, run_start_date=2024-07-03 20:41:02.168093+00:00, run_end_date=2024-07-03 20:41:05.664697+00:00, run_duration=3.496604, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:41:01.384205+00:00, queued_by_job_id=1, pid=193263[0m
[[34m2024-07-03T17:41:05.924-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:40:00+00:00: scheduled__2024-07-03T20:40:00+00:00, state:running, queued_at: 2024-07-03 20:41:01.357441+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:41:05.924-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:40:00+00:00, run_id=scheduled__2024-07-03T20:40:00+00:00, run_start_date=2024-07-03 20:41:01.368454+00:00, run_end_date=2024-07-03 20:41:05.924555+00:00, run_duration=4.556101, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:40:00+00:00, data_interval_end=2024-07-03 20:41:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:41:05.929-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:41:00+00:00, run_after=2024-07-03 20:42:00+00:00[0m
[[34m2024-07-03T17:42:01.934-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:42:00+00:00, run_after=2024-07-03 20:43:00+00:00[0m
[[34m2024-07-03T17:42:01.970-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:41:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:42:01.971-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:42:01.971-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:41:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:42:01.972-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:42:01.972-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:41:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:42:01.973-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:41:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:42:01.977-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:41:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:42:02.652-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:41:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:42:02.726-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:41:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:42:06.607-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:41:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:42:06.610-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:41:00+00:00, map_index=-1, run_start_date=2024-07-03 20:42:02.764173+00:00, run_end_date=2024-07-03 20:42:06.384591+00:00, run_duration=3.620418, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:42:01.971755+00:00, queued_by_job_id=1, pid=193670[0m
[[34m2024-07-03T17:42:06.637-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:41:00+00:00: scheduled__2024-07-03T20:41:00+00:00, state:running, queued_at: 2024-07-03 20:42:01.930423+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:42:06.637-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:41:00+00:00, run_id=scheduled__2024-07-03T20:41:00+00:00, run_start_date=2024-07-03 20:42:01.944457+00:00, run_end_date=2024-07-03 20:42:06.637860+00:00, run_duration=4.693403, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:41:00+00:00, data_interval_end=2024-07-03 20:42:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:42:06.639-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:42:00+00:00, run_after=2024-07-03 20:43:00+00:00[0m
[[34m2024-07-03T17:42:08.740-0300[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-03T17:43:01.397-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:43:00+00:00, run_after=2024-07-03 20:44:00+00:00[0m
[[34m2024-07-03T17:43:01.424-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:42:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:43:01.424-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:43:01.425-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:42:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:43:01.426-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:43:01.426-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:42:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:43:01.426-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:42:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:43:01.430-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:42:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:43:02.381-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:42:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:43:02.497-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:42:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:43:06.340-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:42:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:43:06.347-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:42:00+00:00, map_index=-1, run_start_date=2024-07-03 20:43:02.539151+00:00, run_end_date=2024-07-03 20:43:06.034212+00:00, run_duration=3.495061, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:43:01.425621+00:00, queued_by_job_id=1, pid=194029[0m
[[34m2024-07-03T17:43:06.513-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:42:00+00:00: scheduled__2024-07-03T20:42:00+00:00, state:running, queued_at: 2024-07-03 20:43:01.393433+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:43:06.514-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:42:00+00:00, run_id=scheduled__2024-07-03T20:42:00+00:00, run_start_date=2024-07-03 20:43:01.406175+00:00, run_end_date=2024-07-03 20:43:06.513983+00:00, run_duration=5.107808, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:42:00+00:00, data_interval_end=2024-07-03 20:43:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:43:06.521-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:43:00+00:00, run_after=2024-07-03 20:44:00+00:00[0m
[[34m2024-07-03T17:44:01.165-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:44:00+00:00, run_after=2024-07-03 20:45:00+00:00[0m
[[34m2024-07-03T17:44:01.229-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:43:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:44:01.229-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:44:01.230-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:43:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:44:01.232-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:44:01.233-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:43:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:44:01.233-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:43:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:44:01.237-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:43:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:44:01.983-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:43:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:44:02.068-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:43:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:44:05.563-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:43:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:44:05.568-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:43:00+00:00, map_index=-1, run_start_date=2024-07-03 20:44:02.112353+00:00, run_end_date=2024-07-03 20:44:05.301236+00:00, run_duration=3.188883, state=success, executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:44:01.231015+00:00, queued_by_job_id=1, pid=194414[0m
[[34m2024-07-03T17:44:05.606-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:43:00+00:00: scheduled__2024-07-03T20:43:00+00:00, state:running, queued_at: 2024-07-03 20:44:01.156141+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:44:05.607-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:43:00+00:00, run_id=scheduled__2024-07-03T20:43:00+00:00, run_start_date=2024-07-03 20:44:01.185287+00:00, run_end_date=2024-07-03 20:44:05.607280+00:00, run_duration=4.421993, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:43:00+00:00, data_interval_end=2024-07-03 20:44:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:44:05.611-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:44:00+00:00, run_after=2024-07-03 20:45:00+00:00[0m
[[34m2024-07-03T17:45:01.038-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:45:00+00:00, run_after=2024-07-03 20:46:00+00:00[0m
[[34m2024-07-03T17:45:01.079-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:44:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:45:01.080-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:45:01.080-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:44:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:45:01.081-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:45:01.082-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:44:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:45:01.082-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:44:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:45:01.087-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:44:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:45:01.774-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:44:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:45:01.847-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:44:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:45:05.488-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:44:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:45:05.493-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:44:00+00:00, map_index=-1, run_start_date=2024-07-03 20:45:01.875959+00:00, run_end_date=2024-07-03 20:45:05.267256+00:00, run_duration=3.391297, state=success, executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:45:01.080905+00:00, queued_by_job_id=1, pid=194827[0m
[[34m2024-07-03T17:45:05.523-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:44:00+00:00: scheduled__2024-07-03T20:44:00+00:00, state:running, queued_at: 2024-07-03 20:45:01.033087+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:45:05.523-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:44:00+00:00, run_id=scheduled__2024-07-03T20:44:00+00:00, run_start_date=2024-07-03 20:45:01.052668+00:00, run_end_date=2024-07-03 20:45:05.523557+00:00, run_duration=4.470889, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:44:00+00:00, data_interval_end=2024-07-03 20:45:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:45:05.526-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:45:00+00:00, run_after=2024-07-03 20:46:00+00:00[0m
[[34m2024-07-03T17:46:02.020-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:46:00+00:00, run_after=2024-07-03 20:47:00+00:00[0m
[[34m2024-07-03T17:46:02.046-0300[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:45:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:46:02.046-0300[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG postgres_to_csv has 0/16 running and queued tasks[0m
[[34m2024-07-03T17:46:02.046-0300[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:45:00+00:00 [scheduled]>[0m
[[34m2024-07-03T17:46:02.048-0300[0m] {[34mtaskinstance.py:[0m2260} WARNING[0m - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved[0m
[[34m2024-07-03T17:46:02.048-0300[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:45:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-03T17:46:02.048-0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:46:02.052-0300[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py'][0m
[[34m2024-07-03T17:46:02.757-0300[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/orchestrate/airflow/dags/postgres_to_csv.py[0m
Changing /home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/logs/dag_id=postgres_to_csv/run_id=scheduled__2024-07-03T20:45:00+00:00/task_id=run_meltano_task permission to 509
[[34m2024-07-03T17:46:02.826-0300[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:45:00+00:00 [queued]> on host gabriela-Inspiron-15-3520[0m
[[34m2024-07-03T17:46:06.381-0300[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-03T17:46:06.387-0300[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:45:00+00:00, map_index=-1, run_start_date=2024-07-03 20:46:02.864214+00:00, run_end_date=2024-07-03 20:46:06.120036+00:00, run_duration=3.255822, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:46:02.047373+00:00, queued_by_job_id=1, pid=195308[0m
[[34m2024-07-03T17:46:06.418-0300[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:45:00+00:00: scheduled__2024-07-03T20:45:00+00:00, state:running, queued_at: 2024-07-03 20:46:02.017385+00:00. externally triggered: False> successful[0m
[[34m2024-07-03T17:46:06.419-0300[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:45:00+00:00, run_id=scheduled__2024-07-03T20:45:00+00:00, run_start_date=2024-07-03 20:46:02.028950+00:00, run_end_date=2024-07-03 20:46:06.419084+00:00, run_duration=4.390134, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:45:00+00:00, data_interval_end=2024-07-03 20:46:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512[0m
[[34m2024-07-03T17:46:06.421-0300[0m] {[34mdag.py:[0m3823} INFO[0m - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:46:00+00:00, run_after=2024-07-03 20:47:00+00:00[0m
[[34m2024-07-03T17:46:17.744-0300[0m] {[34mscheduler_job_runner.py:[0m872} ERROR[0m - Exception when executing SchedulerJob._run_scheduler_loop[0m
Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 987, in _run_scheduler_loop
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1061, in _do_scheduling
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 91, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 100, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1127, in _create_dagruns_for_dags
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/models/dag.py", line 3742, in dags_needing_dagruns
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2024-07-03T17:46:17.760-0300[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 185569. PIDs of all processes in the group: [][0m
[[34m2024-07-03T17:46:17.761-0300[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 185569[0m
[[34m2024-07-03T17:46:17.761-0300[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 185569 as process group is missing.[0m
[[34m2024-07-03T17:46:17.761-0300[0m] {[34mscheduler_job_runner.py:[0m884} INFO[0m - Exited execute loop[0m
[[34m2024-07-03T17:46:17.762-0300[0m] {[34mscheduler_command.py:[0m54} ERROR[0m - Exception when running scheduler job[0m
Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 393, in run_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 422, in execute_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 987, in _run_scheduler_loop
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1061, in _do_scheduling
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 91, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 100, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1127, in _create_dagruns_for_dags
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/models/dag.py", line 3742, in dags_needing_dagruns
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 52, in _run_scheduler_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 395, in run_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 240, in complete_execution
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 332, in _update_in_db
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 2853, in get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)
