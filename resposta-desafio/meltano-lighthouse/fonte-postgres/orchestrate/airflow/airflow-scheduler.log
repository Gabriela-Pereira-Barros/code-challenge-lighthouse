2024-07-03 17:22:07,162 INFO - Task context logging is enabled
2024-07-03 17:22:07,163 INFO - Loaded executor: SequentialExecutor
2024-07-03 17:22:07,181 INFO - Starting the scheduler
2024-07-03 17:22:07,181 INFO - Processing each file at most -1 times
2024-07-03 17:22:07,184 INFO - Launched DagFileProcessorManager with pid: 185569
2024-07-03 17:22:07,185 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-07-03 17:22:07,186 INFO - Configured default timezone UTC
2024-07-03 17:22:07,336 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:22:00+00:00, run_after=2024-07-03 20:23:00+00:00
2024-07-03 17:22:07,423 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:21:00+00:00 [scheduled]>
2024-07-03 17:22:07,424 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:22:07,424 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:21:00+00:00 [scheduled]>
2024-07-03 17:22:07,425 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:22:07,425 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:21:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:22:07,425 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:21:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:22:07,428 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:21:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:22:12,923 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:21:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:22:12,930 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:21:00+00:00, map_index=-1, run_start_date=2024-07-03 20:22:08.136819+00:00, run_end_date=2024-07-03 20:22:12.701099+00:00, run_duration=4.56428, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:22:07.424402+00:00, queued_by_job_id=1, pid=185577
2024-07-03 17:22:14,674 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:21:00+00:00: scheduled__2024-07-03T20:21:00+00:00, state:running, queued_at: 2024-07-03 20:22:07.328561+00:00. externally triggered: False> successful
2024-07-03 17:22:14,675 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:21:00+00:00, run_id=scheduled__2024-07-03T20:21:00+00:00, run_start_date=2024-07-03 20:22:07.401600+00:00, run_end_date=2024-07-03 20:22:14.675453+00:00, run_duration=7.273853, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:21:00+00:00, data_interval_end=2024-07-03 20:22:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:22:14,679 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:22:00+00:00, run_after=2024-07-03 20:23:00+00:00
2024-07-03 17:23:01,662 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:23:00+00:00, run_after=2024-07-03 20:24:00+00:00
2024-07-03 17:23:01,686 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:22:00+00:00 [scheduled]>
2024-07-03 17:23:01,686 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:23:01,686 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:22:00+00:00 [scheduled]>
2024-07-03 17:23:01,687 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:23:01,688 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:22:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:23:01,688 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:22:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:23:01,691 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:22:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:23:06,115 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:22:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:23:06,118 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:22:00+00:00, map_index=-1, run_start_date=2024-07-03 20:23:02.459944+00:00, run_end_date=2024-07-03 20:23:05.858233+00:00, run_duration=3.398289, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:23:01.687243+00:00, queued_by_job_id=1, pid=185918
2024-07-03 17:23:06,157 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:22:00+00:00: scheduled__2024-07-03T20:22:00+00:00, state:running, queued_at: 2024-07-03 20:23:01.658920+00:00. externally triggered: False> successful
2024-07-03 17:23:06,158 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:22:00+00:00, run_id=scheduled__2024-07-03T20:22:00+00:00, run_start_date=2024-07-03 20:23:01.670992+00:00, run_end_date=2024-07-03 20:23:06.158261+00:00, run_duration=4.487269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:22:00+00:00, data_interval_end=2024-07-03 20:23:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:23:06,163 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:23:00+00:00, run_after=2024-07-03 20:24:00+00:00
2024-07-03 17:24:01,376 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:24:00+00:00, run_after=2024-07-03 20:25:00+00:00
2024-07-03 17:24:01,403 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:23:00+00:00 [scheduled]>
2024-07-03 17:24:01,403 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:24:01,403 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:23:00+00:00 [scheduled]>
2024-07-03 17:24:01,404 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:24:01,404 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:23:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:24:01,404 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:23:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:24:01,408 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:23:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:24:05,886 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:23:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:24:05,890 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:23:00+00:00, map_index=-1, run_start_date=2024-07-03 20:24:02.173492+00:00, run_end_date=2024-07-03 20:24:05.617716+00:00, run_duration=3.444224, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:24:01.403966+00:00, queued_by_job_id=1, pid=186304
2024-07-03 17:24:05,930 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:23:00+00:00: scheduled__2024-07-03T20:23:00+00:00, state:running, queued_at: 2024-07-03 20:24:01.372267+00:00. externally triggered: False> successful
2024-07-03 17:24:05,931 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:23:00+00:00, run_id=scheduled__2024-07-03T20:23:00+00:00, run_start_date=2024-07-03 20:24:01.384942+00:00, run_end_date=2024-07-03 20:24:05.931455+00:00, run_duration=4.546513, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:23:00+00:00, data_interval_end=2024-07-03 20:24:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:24:05,938 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:24:00+00:00, run_after=2024-07-03 20:25:00+00:00
2024-07-03 17:25:01,147 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:25:00+00:00, run_after=2024-07-03 20:26:00+00:00
2024-07-03 17:25:01,168 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:24:00+00:00 [scheduled]>
2024-07-03 17:25:01,168 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:25:01,168 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:24:00+00:00 [scheduled]>
2024-07-03 17:25:01,169 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:25:01,169 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:24:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:25:01,169 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:24:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:25:01,172 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:24:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:25:05,478 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:24:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:25:05,481 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:24:00+00:00, map_index=-1, run_start_date=2024-07-03 20:25:01.875170+00:00, run_end_date=2024-07-03 20:25:05.246087+00:00, run_duration=3.370917, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:25:01.169032+00:00, queued_by_job_id=1, pid=186716
2024-07-03 17:25:05,517 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:24:00+00:00: scheduled__2024-07-03T20:24:00+00:00, state:running, queued_at: 2024-07-03 20:25:01.145000+00:00. externally triggered: False> successful
2024-07-03 17:25:05,517 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:24:00+00:00, run_id=scheduled__2024-07-03T20:24:00+00:00, run_start_date=2024-07-03 20:25:01.155259+00:00, run_end_date=2024-07-03 20:25:05.517537+00:00, run_duration=4.362278, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:24:00+00:00, data_interval_end=2024-07-03 20:25:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:25:05,522 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:25:00+00:00, run_after=2024-07-03 20:26:00+00:00
2024-07-03 17:26:02,015 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:26:00+00:00, run_after=2024-07-03 20:27:00+00:00
2024-07-03 17:26:02,037 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:25:00+00:00 [scheduled]>
2024-07-03 17:26:02,037 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:26:02,038 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:25:00+00:00 [scheduled]>
2024-07-03 17:26:02,038 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:26:02,038 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:25:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:26:02,039 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:26:02,042 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:26:06,602 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:25:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:26:06,606 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:25:00+00:00, map_index=-1, run_start_date=2024-07-03 20:26:02.824424+00:00, run_end_date=2024-07-03 20:26:06.312896+00:00, run_duration=3.488472, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:26:02.038306+00:00, queued_by_job_id=1, pid=187086
2024-07-03 17:26:06,637 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:25:00+00:00: scheduled__2024-07-03T20:25:00+00:00, state:running, queued_at: 2024-07-03 20:26:02.011734+00:00. externally triggered: False> successful
2024-07-03 17:26:06,637 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:25:00+00:00, run_id=scheduled__2024-07-03T20:25:00+00:00, run_start_date=2024-07-03 20:26:02.023435+00:00, run_end_date=2024-07-03 20:26:06.637326+00:00, run_duration=4.613891, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:25:00+00:00, data_interval_end=2024-07-03 20:26:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:26:06,639 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:26:00+00:00, run_after=2024-07-03 20:27:00+00:00
2024-07-03 17:27:01,118 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:27:00+00:00, run_after=2024-07-03 20:28:00+00:00
2024-07-03 17:27:01,154 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:26:00+00:00 [scheduled]>
2024-07-03 17:27:01,154 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:27:01,155 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:26:00+00:00 [scheduled]>
2024-07-03 17:27:01,156 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:27:01,156 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:26:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:27:01,157 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:26:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:27:01,160 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:26:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:27:05,850 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:26:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:27:05,855 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:26:00+00:00, map_index=-1, run_start_date=2024-07-03 20:27:01.931988+00:00, run_end_date=2024-07-03 20:27:05.606012+00:00, run_duration=3.674024, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:27:01.155535+00:00, queued_by_job_id=1, pid=187460
2024-07-03 17:27:07,702 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:26:00+00:00: scheduled__2024-07-03T20:26:00+00:00, state:running, queued_at: 2024-07-03 20:27:01.113823+00:00. externally triggered: False> successful
2024-07-03 17:27:07,703 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:26:00+00:00, run_id=scheduled__2024-07-03T20:26:00+00:00, run_start_date=2024-07-03 20:27:01.131736+00:00, run_end_date=2024-07-03 20:27:07.702934+00:00, run_duration=6.571198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:26:00+00:00, data_interval_end=2024-07-03 20:27:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:27:07,707 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:27:00+00:00, run_after=2024-07-03 20:28:00+00:00
2024-07-03 17:27:07,732 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-07-03 17:28:01,256 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:28:00+00:00, run_after=2024-07-03 20:29:00+00:00
2024-07-03 17:28:01,281 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:27:00+00:00 [scheduled]>
2024-07-03 17:28:01,282 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:28:01,282 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:27:00+00:00 [scheduled]>
2024-07-03 17:28:01,283 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:28:01,283 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:27:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:28:01,283 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:27:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:28:01,287 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:27:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:28:08,463 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:27:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:28:08,470 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:27:00+00:00, map_index=-1, run_start_date=2024-07-03 20:28:02.113060+00:00, run_end_date=2024-07-03 20:28:08.204802+00:00, run_duration=6.091742, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:28:01.282556+00:00, queued_by_job_id=1, pid=188009
2024-07-03 17:28:08,523 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:27:00+00:00: scheduled__2024-07-03T20:27:00+00:00, state:running, queued_at: 2024-07-03 20:28:01.251918+00:00. externally triggered: False> successful
2024-07-03 17:28:08,524 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:27:00+00:00, run_id=scheduled__2024-07-03T20:27:00+00:00, run_start_date=2024-07-03 20:28:01.265222+00:00, run_end_date=2024-07-03 20:28:08.524298+00:00, run_duration=7.259076, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:27:00+00:00, data_interval_end=2024-07-03 20:28:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:28:08,528 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:28:00+00:00, run_after=2024-07-03 20:29:00+00:00
2024-07-03 17:29:01,344 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:29:00+00:00, run_after=2024-07-03 20:30:00+00:00
2024-07-03 17:29:01,374 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:28:00+00:00 [scheduled]>
2024-07-03 17:29:01,374 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:29:01,374 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:28:00+00:00 [scheduled]>
2024-07-03 17:29:01,375 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:29:01,376 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:28:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:29:01,376 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:28:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:29:01,379 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:28:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:29:06,224 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:28:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:29:06,229 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:28:00+00:00, map_index=-1, run_start_date=2024-07-03 20:29:02.360874+00:00, run_end_date=2024-07-03 20:29:05.958180+00:00, run_duration=3.597306, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:29:01.375058+00:00, queued_by_job_id=1, pid=188499
2024-07-03 17:29:06,270 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:28:00+00:00: scheduled__2024-07-03T20:28:00+00:00, state:running, queued_at: 2024-07-03 20:29:01.338877+00:00. externally triggered: False> successful
2024-07-03 17:29:06,271 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:28:00+00:00, run_id=scheduled__2024-07-03T20:28:00+00:00, run_start_date=2024-07-03 20:29:01.354421+00:00, run_end_date=2024-07-03 20:29:06.270943+00:00, run_duration=4.916522, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:28:00+00:00, data_interval_end=2024-07-03 20:29:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:29:06,275 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:29:00+00:00, run_after=2024-07-03 20:30:00+00:00
2024-07-03 17:30:01,622 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:30:00+00:00, run_after=2024-07-03 20:31:00+00:00
2024-07-03 17:30:01,650 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:29:00+00:00 [scheduled]>
2024-07-03 17:30:01,651 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:30:01,651 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:29:00+00:00 [scheduled]>
2024-07-03 17:30:01,652 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:30:01,652 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:29:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:30:01,652 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:29:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:30:01,656 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:29:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:30:06,438 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:29:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:30:06,447 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:29:00+00:00, map_index=-1, run_start_date=2024-07-03 20:30:02.554337+00:00, run_end_date=2024-07-03 20:30:06.117495+00:00, run_duration=3.563158, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:30:01.651835+00:00, queued_by_job_id=1, pid=188867
2024-07-03 17:30:06,484 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:29:00+00:00: scheduled__2024-07-03T20:29:00+00:00, state:running, queued_at: 2024-07-03 20:30:01.618664+00:00. externally triggered: False> successful
2024-07-03 17:30:06,484 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:29:00+00:00, run_id=scheduled__2024-07-03T20:29:00+00:00, run_start_date=2024-07-03 20:30:01.632584+00:00, run_end_date=2024-07-03 20:30:06.484564+00:00, run_duration=4.85198, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:29:00+00:00, data_interval_end=2024-07-03 20:30:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:30:06,486 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:30:00+00:00, run_after=2024-07-03 20:31:00+00:00
2024-07-03 17:31:01,777 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:31:00+00:00, run_after=2024-07-03 20:32:00+00:00
2024-07-03 17:31:01,807 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:30:00+00:00 [scheduled]>
2024-07-03 17:31:01,808 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:31:01,808 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:30:00+00:00 [scheduled]>
2024-07-03 17:31:01,809 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:31:01,810 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:31:01,810 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:31:01,814 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:31:06,754 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:30:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:31:06,761 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:30:00+00:00, map_index=-1, run_start_date=2024-07-03 20:31:02.756377+00:00, run_end_date=2024-07-03 20:31:06.532014+00:00, run_duration=3.775637, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:31:01.809042+00:00, queued_by_job_id=1, pid=189302
2024-07-03 17:31:06,807 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:30:00+00:00: scheduled__2024-07-03T20:30:00+00:00, state:running, queued_at: 2024-07-03 20:31:01.774298+00:00. externally triggered: False> successful
2024-07-03 17:31:06,807 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:30:00+00:00, run_id=scheduled__2024-07-03T20:30:00+00:00, run_start_date=2024-07-03 20:31:01.785597+00:00, run_end_date=2024-07-03 20:31:06.807536+00:00, run_duration=5.021939, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:30:00+00:00, data_interval_end=2024-07-03 20:31:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:31:06,812 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:31:00+00:00, run_after=2024-07-03 20:32:00+00:00
2024-07-03 17:32:01,783 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:32:00+00:00, run_after=2024-07-03 20:33:00+00:00
2024-07-03 17:32:01,840 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:31:00+00:00 [scheduled]>
2024-07-03 17:32:01,840 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:32:01,841 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:31:00+00:00 [scheduled]>
2024-07-03 17:32:01,842 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:32:01,843 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:31:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:32:01,843 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:31:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:32:01,847 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:31:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:32:07,317 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:31:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:32:07,321 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:31:00+00:00, map_index=-1, run_start_date=2024-07-03 20:32:02.904493+00:00, run_end_date=2024-07-03 20:32:07.042276+00:00, run_duration=4.137783, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:32:01.841824+00:00, queued_by_job_id=1, pid=189650
2024-07-03 17:32:07,351 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:31:00+00:00: scheduled__2024-07-03T20:31:00+00:00, state:running, queued_at: 2024-07-03 20:32:01.769517+00:00. externally triggered: False> successful
2024-07-03 17:32:07,351 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:31:00+00:00, run_id=scheduled__2024-07-03T20:31:00+00:00, run_start_date=2024-07-03 20:32:01.798363+00:00, run_end_date=2024-07-03 20:32:07.351757+00:00, run_duration=5.553394, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:31:00+00:00, data_interval_end=2024-07-03 20:32:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:32:07,354 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:32:00+00:00, run_after=2024-07-03 20:33:00+00:00
2024-07-03 17:32:07,773 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-07-03 17:33:01,932 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:33:00+00:00, run_after=2024-07-03 20:34:00+00:00
2024-07-03 17:33:02,000 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:32:00+00:00 [scheduled]>
2024-07-03 17:33:02,001 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:33:02,001 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:32:00+00:00 [scheduled]>
2024-07-03 17:33:02,003 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:33:02,004 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:32:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:33:02,004 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:32:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:33:02,011 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:32:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:33:06,755 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:32:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:33:06,760 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:32:00+00:00, map_index=-1, run_start_date=2024-07-03 20:33:03.001101+00:00, run_end_date=2024-07-03 20:33:06.481180+00:00, run_duration=3.480079, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:33:02.002080+00:00, queued_by_job_id=1, pid=190011
2024-07-03 17:33:06,789 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:32:00+00:00: scheduled__2024-07-03T20:32:00+00:00, state:running, queued_at: 2024-07-03 20:33:01.909468+00:00. externally triggered: False> successful
2024-07-03 17:33:06,789 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:32:00+00:00, run_id=scheduled__2024-07-03T20:32:00+00:00, run_start_date=2024-07-03 20:33:01.972793+00:00, run_end_date=2024-07-03 20:33:06.789466+00:00, run_duration=4.816673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:32:00+00:00, data_interval_end=2024-07-03 20:33:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:33:06,791 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:33:00+00:00, run_after=2024-07-03 20:34:00+00:00
2024-07-03 17:34:01,317 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:34:00+00:00, run_after=2024-07-03 20:35:00+00:00
2024-07-03 17:34:01,354 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:33:00+00:00 [scheduled]>
2024-07-03 17:34:01,354 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:34:01,355 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:33:00+00:00 [scheduled]>
2024-07-03 17:34:01,356 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:34:01,357 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:33:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:34:01,357 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:33:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:34:01,360 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:33:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:34:07,484 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:33:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:34:07,490 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:33:00+00:00, map_index=-1, run_start_date=2024-07-03 20:34:02.951266+00:00, run_end_date=2024-07-03 20:34:07.270547+00:00, run_duration=4.319281, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:34:01.355622+00:00, queued_by_job_id=1, pid=190403
2024-07-03 17:34:07,517 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:33:00+00:00: scheduled__2024-07-03T20:33:00+00:00, state:running, queued_at: 2024-07-03 20:34:01.311015+00:00. externally triggered: False> successful
2024-07-03 17:34:07,517 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:33:00+00:00, run_id=scheduled__2024-07-03T20:33:00+00:00, run_start_date=2024-07-03 20:34:01.329040+00:00, run_end_date=2024-07-03 20:34:07.517676+00:00, run_duration=6.188636, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:33:00+00:00, data_interval_end=2024-07-03 20:34:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:34:07,519 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:34:00+00:00, run_after=2024-07-03 20:35:00+00:00
2024-07-03 17:35:01,203 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:35:00+00:00, run_after=2024-07-03 20:36:00+00:00
2024-07-03 17:35:01,237 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:34:00+00:00 [scheduled]>
2024-07-03 17:35:01,237 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:35:01,237 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:34:00+00:00 [scheduled]>
2024-07-03 17:35:01,239 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:35:01,239 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:34:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:35:01,239 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:34:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:35:01,243 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:34:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:35:05,719 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:34:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:35:05,724 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:34:00+00:00, map_index=-1, run_start_date=2024-07-03 20:35:02.071252+00:00, run_end_date=2024-07-03 20:35:05.387400+00:00, run_duration=3.316148, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:35:01.238291+00:00, queued_by_job_id=1, pid=190822
2024-07-03 17:35:05,761 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:34:00+00:00: scheduled__2024-07-03T20:34:00+00:00, state:running, queued_at: 2024-07-03 20:35:01.198852+00:00. externally triggered: False> successful
2024-07-03 17:35:05,761 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:34:00+00:00, run_id=scheduled__2024-07-03T20:34:00+00:00, run_start_date=2024-07-03 20:35:01.213622+00:00, run_end_date=2024-07-03 20:35:05.761372+00:00, run_duration=4.54775, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:34:00+00:00, data_interval_end=2024-07-03 20:35:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:35:05,764 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:35:00+00:00, run_after=2024-07-03 20:36:00+00:00
2024-07-03 17:36:02,934 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:36:00+00:00, run_after=2024-07-03 20:37:00+00:00
2024-07-03 17:36:02,965 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:35:00+00:00 [scheduled]>
2024-07-03 17:36:02,966 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:36:02,966 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:35:00+00:00 [scheduled]>
2024-07-03 17:36:02,968 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:36:02,968 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:35:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:36:02,968 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:36:02,972 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:36:07,838 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:35:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:36:07,842 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:35:00+00:00, map_index=-1, run_start_date=2024-07-03 20:36:03.854899+00:00, run_end_date=2024-07-03 20:36:07.611874+00:00, run_duration=3.756975, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:36:02.967153+00:00, queued_by_job_id=1, pid=191210
2024-07-03 17:36:07,981 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:35:00+00:00: scheduled__2024-07-03T20:35:00+00:00, state:running, queued_at: 2024-07-03 20:36:02.931270+00:00. externally triggered: False> successful
2024-07-03 17:36:07,981 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:35:00+00:00, run_id=scheduled__2024-07-03T20:35:00+00:00, run_start_date=2024-07-03 20:36:02.942229+00:00, run_end_date=2024-07-03 20:36:07.981666+00:00, run_duration=5.039437, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:35:00+00:00, data_interval_end=2024-07-03 20:36:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:36:07,985 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:36:00+00:00, run_after=2024-07-03 20:37:00+00:00
2024-07-03 17:37:01,175 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:37:00+00:00, run_after=2024-07-03 20:38:00+00:00
2024-07-03 17:37:01,204 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:36:00+00:00 [scheduled]>
2024-07-03 17:37:01,204 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:37:01,204 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:36:00+00:00 [scheduled]>
2024-07-03 17:37:01,205 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:37:01,206 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:36:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:37:01,206 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:36:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:37:01,209 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:36:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:37:06,504 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:36:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:37:06,509 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:36:00+00:00, map_index=-1, run_start_date=2024-07-03 20:37:02.117707+00:00, run_end_date=2024-07-03 20:37:06.220503+00:00, run_duration=4.102796, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:37:01.204939+00:00, queued_by_job_id=1, pid=191563
2024-07-03 17:37:08,667 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:36:00+00:00: scheduled__2024-07-03T20:36:00+00:00, state:running, queued_at: 2024-07-03 20:37:01.171999+00:00. externally triggered: False> successful
2024-07-03 17:37:08,668 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:36:00+00:00, run_id=scheduled__2024-07-03T20:36:00+00:00, run_start_date=2024-07-03 20:37:01.186006+00:00, run_end_date=2024-07-03 20:37:08.667978+00:00, run_duration=7.481972, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:36:00+00:00, data_interval_end=2024-07-03 20:37:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:37:08,675 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:37:00+00:00, run_after=2024-07-03 20:38:00+00:00
2024-07-03 17:37:08,698 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-07-03 17:38:02,050 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:38:00+00:00, run_after=2024-07-03 20:39:00+00:00
2024-07-03 17:38:02,096 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:37:00+00:00 [scheduled]>
2024-07-03 17:38:02,096 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:38:02,097 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:37:00+00:00 [scheduled]>
2024-07-03 17:38:02,098 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:38:02,099 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:37:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:38:02,099 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:37:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:38:02,103 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:37:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:38:06,703 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:37:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:38:06,706 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:37:00+00:00, map_index=-1, run_start_date=2024-07-03 20:38:02.931568+00:00, run_end_date=2024-07-03 20:38:06.502882+00:00, run_duration=3.571314, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:38:02.097703+00:00, queued_by_job_id=1, pid=191974
2024-07-03 17:38:06,736 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:37:00+00:00: scheduled__2024-07-03T20:37:00+00:00, state:running, queued_at: 2024-07-03 20:38:02.044198+00:00. externally triggered: False> successful
2024-07-03 17:38:06,737 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:37:00+00:00, run_id=scheduled__2024-07-03T20:37:00+00:00, run_start_date=2024-07-03 20:38:02.065309+00:00, run_end_date=2024-07-03 20:38:06.737183+00:00, run_duration=4.671874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:37:00+00:00, data_interval_end=2024-07-03 20:38:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:38:06,741 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:38:00+00:00, run_after=2024-07-03 20:39:00+00:00
2024-07-03 17:39:01,145 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:39:00+00:00, run_after=2024-07-03 20:40:00+00:00
2024-07-03 17:39:01,183 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:38:00+00:00 [scheduled]>
2024-07-03 17:39:01,184 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:39:01,184 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:38:00+00:00 [scheduled]>
2024-07-03 17:39:01,185 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:39:01,185 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:38:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:39:01,185 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:38:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:39:01,188 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:38:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:39:05,318 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:38:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:39:05,322 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:38:00+00:00, map_index=-1, run_start_date=2024-07-03 20:39:01.903432+00:00, run_end_date=2024-07-03 20:39:05.131304+00:00, run_duration=3.227872, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:39:01.184685+00:00, queued_by_job_id=1, pid=192456
2024-07-03 17:39:05,351 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:38:00+00:00: scheduled__2024-07-03T20:38:00+00:00, state:running, queued_at: 2024-07-03 20:39:01.138612+00:00. externally triggered: False> successful
2024-07-03 17:39:05,351 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:38:00+00:00, run_id=scheduled__2024-07-03T20:38:00+00:00, run_start_date=2024-07-03 20:39:01.159885+00:00, run_end_date=2024-07-03 20:39:05.351583+00:00, run_duration=4.191698, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:38:00+00:00, data_interval_end=2024-07-03 20:39:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:39:05,353 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:39:00+00:00, run_after=2024-07-03 20:40:00+00:00
2024-07-03 17:40:01,239 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:40:00+00:00, run_after=2024-07-03 20:41:00+00:00
2024-07-03 17:40:01,268 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:39:00+00:00 [scheduled]>
2024-07-03 17:40:01,268 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:40:01,269 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:39:00+00:00 [scheduled]>
2024-07-03 17:40:01,271 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:40:01,271 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:39:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:40:01,272 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:39:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:40:01,276 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:39:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:40:05,721 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:39:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:40:05,725 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:39:00+00:00, map_index=-1, run_start_date=2024-07-03 20:40:02.039449+00:00, run_end_date=2024-07-03 20:40:05.458873+00:00, run_duration=3.419424, state=success, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:40:01.269965+00:00, queued_by_job_id=1, pid=192854
2024-07-03 17:40:05,764 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:39:00+00:00: scheduled__2024-07-03T20:39:00+00:00, state:running, queued_at: 2024-07-03 20:40:01.234692+00:00. externally triggered: False> successful
2024-07-03 17:40:05,765 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:39:00+00:00, run_id=scheduled__2024-07-03T20:39:00+00:00, run_start_date=2024-07-03 20:40:01.248938+00:00, run_end_date=2024-07-03 20:40:05.765198+00:00, run_duration=4.51626, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:39:00+00:00, data_interval_end=2024-07-03 20:40:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:40:05,770 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:40:00+00:00, run_after=2024-07-03 20:41:00+00:00
2024-07-03 17:41:01,361 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:41:00+00:00, run_after=2024-07-03 20:42:00+00:00
2024-07-03 17:41:01,383 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:40:00+00:00 [scheduled]>
2024-07-03 17:41:01,383 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:41:01,383 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:40:00+00:00 [scheduled]>
2024-07-03 17:41:01,384 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:41:01,385 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:41:01,385 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:41:01,388 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:41:05,880 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:40:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:41:05,883 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:40:00+00:00, map_index=-1, run_start_date=2024-07-03 20:41:02.168093+00:00, run_end_date=2024-07-03 20:41:05.664697+00:00, run_duration=3.496604, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:41:01.384205+00:00, queued_by_job_id=1, pid=193263
2024-07-03 17:41:05,924 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:40:00+00:00: scheduled__2024-07-03T20:40:00+00:00, state:running, queued_at: 2024-07-03 20:41:01.357441+00:00. externally triggered: False> successful
2024-07-03 17:41:05,924 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:40:00+00:00, run_id=scheduled__2024-07-03T20:40:00+00:00, run_start_date=2024-07-03 20:41:01.368454+00:00, run_end_date=2024-07-03 20:41:05.924555+00:00, run_duration=4.556101, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:40:00+00:00, data_interval_end=2024-07-03 20:41:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:41:05,929 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:41:00+00:00, run_after=2024-07-03 20:42:00+00:00
2024-07-03 17:42:01,934 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:42:00+00:00, run_after=2024-07-03 20:43:00+00:00
2024-07-03 17:42:01,970 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:41:00+00:00 [scheduled]>
2024-07-03 17:42:01,971 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:42:01,971 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:41:00+00:00 [scheduled]>
2024-07-03 17:42:01,972 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:42:01,972 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:41:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:42:01,973 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:41:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:42:01,977 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:41:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:42:06,607 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:41:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:42:06,610 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:41:00+00:00, map_index=-1, run_start_date=2024-07-03 20:42:02.764173+00:00, run_end_date=2024-07-03 20:42:06.384591+00:00, run_duration=3.620418, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:42:01.971755+00:00, queued_by_job_id=1, pid=193670
2024-07-03 17:42:06,637 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:41:00+00:00: scheduled__2024-07-03T20:41:00+00:00, state:running, queued_at: 2024-07-03 20:42:01.930423+00:00. externally triggered: False> successful
2024-07-03 17:42:06,637 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:41:00+00:00, run_id=scheduled__2024-07-03T20:41:00+00:00, run_start_date=2024-07-03 20:42:01.944457+00:00, run_end_date=2024-07-03 20:42:06.637860+00:00, run_duration=4.693403, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:41:00+00:00, data_interval_end=2024-07-03 20:42:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:42:06,639 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:42:00+00:00, run_after=2024-07-03 20:43:00+00:00
2024-07-03 17:42:08,740 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-07-03 17:43:01,397 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:43:00+00:00, run_after=2024-07-03 20:44:00+00:00
2024-07-03 17:43:01,424 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:42:00+00:00 [scheduled]>
2024-07-03 17:43:01,424 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:43:01,425 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:42:00+00:00 [scheduled]>
2024-07-03 17:43:01,426 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:43:01,426 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:42:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:43:01,426 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:42:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:43:01,430 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:42:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:43:06,340 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:42:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:43:06,347 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:42:00+00:00, map_index=-1, run_start_date=2024-07-03 20:43:02.539151+00:00, run_end_date=2024-07-03 20:43:06.034212+00:00, run_duration=3.495061, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:43:01.425621+00:00, queued_by_job_id=1, pid=194029
2024-07-03 17:43:06,513 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:42:00+00:00: scheduled__2024-07-03T20:42:00+00:00, state:running, queued_at: 2024-07-03 20:43:01.393433+00:00. externally triggered: False> successful
2024-07-03 17:43:06,514 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:42:00+00:00, run_id=scheduled__2024-07-03T20:42:00+00:00, run_start_date=2024-07-03 20:43:01.406175+00:00, run_end_date=2024-07-03 20:43:06.513983+00:00, run_duration=5.107808, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:42:00+00:00, data_interval_end=2024-07-03 20:43:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:43:06,521 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:43:00+00:00, run_after=2024-07-03 20:44:00+00:00
2024-07-03 17:44:01,165 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:44:00+00:00, run_after=2024-07-03 20:45:00+00:00
2024-07-03 17:44:01,229 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:43:00+00:00 [scheduled]>
2024-07-03 17:44:01,229 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:44:01,230 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:43:00+00:00 [scheduled]>
2024-07-03 17:44:01,232 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:44:01,233 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:43:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:44:01,233 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:43:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:44:01,237 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:43:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:44:05,563 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:43:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:44:05,568 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:43:00+00:00, map_index=-1, run_start_date=2024-07-03 20:44:02.112353+00:00, run_end_date=2024-07-03 20:44:05.301236+00:00, run_duration=3.188883, state=success, executor_state=success, try_number=1, max_tries=0, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:44:01.231015+00:00, queued_by_job_id=1, pid=194414
2024-07-03 17:44:05,606 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:43:00+00:00: scheduled__2024-07-03T20:43:00+00:00, state:running, queued_at: 2024-07-03 20:44:01.156141+00:00. externally triggered: False> successful
2024-07-03 17:44:05,607 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:43:00+00:00, run_id=scheduled__2024-07-03T20:43:00+00:00, run_start_date=2024-07-03 20:44:01.185287+00:00, run_end_date=2024-07-03 20:44:05.607280+00:00, run_duration=4.421993, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:43:00+00:00, data_interval_end=2024-07-03 20:44:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:44:05,611 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:44:00+00:00, run_after=2024-07-03 20:45:00+00:00
2024-07-03 17:45:01,038 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:45:00+00:00, run_after=2024-07-03 20:46:00+00:00
2024-07-03 17:45:01,079 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:44:00+00:00 [scheduled]>
2024-07-03 17:45:01,080 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:45:01,080 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:44:00+00:00 [scheduled]>
2024-07-03 17:45:01,081 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:45:01,082 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:44:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:45:01,082 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:44:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:45:01,087 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:44:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:45:05,488 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:44:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:45:05,493 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:44:00+00:00, map_index=-1, run_start_date=2024-07-03 20:45:01.875959+00:00, run_end_date=2024-07-03 20:45:05.267256+00:00, run_duration=3.391297, state=success, executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:45:01.080905+00:00, queued_by_job_id=1, pid=194827
2024-07-03 17:45:05,523 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:44:00+00:00: scheduled__2024-07-03T20:44:00+00:00, state:running, queued_at: 2024-07-03 20:45:01.033087+00:00. externally triggered: False> successful
2024-07-03 17:45:05,523 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:44:00+00:00, run_id=scheduled__2024-07-03T20:44:00+00:00, run_start_date=2024-07-03 20:45:01.052668+00:00, run_end_date=2024-07-03 20:45:05.523557+00:00, run_duration=4.470889, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:44:00+00:00, data_interval_end=2024-07-03 20:45:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:45:05,526 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:45:00+00:00, run_after=2024-07-03 20:46:00+00:00
2024-07-03 17:46:02,020 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:46:00+00:00, run_after=2024-07-03 20:47:00+00:00
2024-07-03 17:46:02,046 INFO - 1 tasks up for execution:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:45:00+00:00 [scheduled]>
2024-07-03 17:46:02,046 INFO - DAG postgres_to_csv has 0/16 running and queued tasks
2024-07-03 17:46:02,046 INFO - Setting the following tasks to queued state:
	<TaskInstance: postgres_to_csv.run_meltano_task scheduled__2024-07-03T20:45:00+00:00 [scheduled]>
2024-07-03 17:46:02,048 WARNING - cannot record scheduled_duration for task run_meltano_task because previous state change time has not been saved
2024-07-03 17:46:02,048 INFO - Sending TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:45:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-07-03 17:46:02,048 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:46:02,052 INFO - Executing command: ['airflow', 'tasks', 'run', 'postgres_to_csv', 'run_meltano_task', 'scheduled__2024-07-03T20:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/postgres_to_csv.py']
2024-07-03 17:46:06,381 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='postgres_to_csv', task_id='run_meltano_task', run_id='scheduled__2024-07-03T20:45:00+00:00', try_number=1, map_index=-1)
2024-07-03 17:46:06,387 INFO - TaskInstance Finished: dag_id=postgres_to_csv, task_id=run_meltano_task, run_id=scheduled__2024-07-03T20:45:00+00:00, map_index=-1, run_start_date=2024-07-03 20:46:02.864214+00:00, run_end_date=2024-07-03 20:46:06.120036+00:00, run_duration=3.255822, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-03 20:46:02.047373+00:00, queued_by_job_id=1, pid=195308
2024-07-03 17:46:06,418 INFO - Marking run <DagRun postgres_to_csv @ 2024-07-03 20:45:00+00:00: scheduled__2024-07-03T20:45:00+00:00, state:running, queued_at: 2024-07-03 20:46:02.017385+00:00. externally triggered: False> successful
2024-07-03 17:46:06,419 INFO - DagRun Finished: dag_id=postgres_to_csv, execution_date=2024-07-03 20:45:00+00:00, run_id=scheduled__2024-07-03T20:45:00+00:00, run_start_date=2024-07-03 20:46:02.028950+00:00, run_end_date=2024-07-03 20:46:06.419084+00:00, run_duration=4.390134, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 20:45:00+00:00, data_interval_end=2024-07-03 20:46:00+00:00, dag_hash=01b401be40dbe8fe745c38e1587b0512
2024-07-03 17:46:06,421 INFO - Setting next_dagrun for postgres_to_csv to 2024-07-03 20:46:00+00:00, run_after=2024-07-03 20:47:00+00:00
2024-07-03 17:46:17,744 ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 987, in _run_scheduler_loop
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1061, in _do_scheduling
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 91, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 100, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1127, in _create_dagruns_for_dags
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/models/dag.py", line 3742, in dags_needing_dagruns
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)
2024-07-03 17:46:17,760 INFO - Sending Signals.SIGTERM to group 185569. PIDs of all processes in the group: []
2024-07-03 17:46:17,761 INFO - Sending the signal Signals.SIGTERM to group 185569
2024-07-03 17:46:17,761 INFO - Sending the signal Signals.SIGTERM to process 185569 as process group is missing.
2024-07-03 17:46:17,761 INFO - Exited execute loop
2024-07-03 17:46:17,762 ERROR - Exception when running scheduler job
Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 393, in run_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 422, in execute_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 987, in _run_scheduler_loop
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1061, in _do_scheduling
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 91, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 347, in __iter__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 325, in iter
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/tenacity/__init__.py", line 158, in reraise
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gabriela/anaconda3/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/retries.py", line 100, in wrapped_function
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 1127, in _create_dagruns_for_dags
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/models/dag.py", line 3742, in dags_needing_dagruns
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 52, in _run_scheduler_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 79, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 395, in run_job
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 240, in complete_execution
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/utils/session.py", line 76, in wrapper
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/airflow/jobs/job.py", line 332, in _update_in_db
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 2853, in get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
  File "/home/gabriela/Documents/meltano-lighthouse/fonte-postgres/.meltano/utilities/airflow/venv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)
